{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display,HTML\n",
    "def dhtml(str):\n",
    "    display(HTML(\"\"\"<style>\n",
    "    @import 'https://fonts.googleapis.com/css?family=Akronim&effect=3d';      \n",
    "    </style><h1 class='font-effect-3d' \n",
    "    style='font-family:Akronim; color:#33ffcc;'>\n",
    "    %s</h1>\"\"\"%str))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Reading classics [Deep Learning Models](https://nbviewer.jupyter.org/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/rnn/rnn_lstm_packed_imdb.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @import 'https://fonts.googleapis.com/css?family=Akronim&effect=3d';      \n",
       "    </style><h1 class='font-effect-3d' \n",
       "    style='font-family:Akronim; color:#33ffcc;'>\n",
       "    Code Modules, Functions, & Classes</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dhtml('Code Modules, Functions, & Classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np,pylab as pl\n",
    "import torch,random,spacy,pandas as pd\n",
    "import torch.nn as tnn\n",
    "import torch.nn.functional as tnnf\n",
    "from torchtext import data as ttdata\n",
    "from torchtext import datasets as ttds\n",
    "torch.backends.cudnn.deterministic=True\n",
    "dev=torch.device('cuda' \\\n",
    "if torch.cuda.is_available() else 'cpu')\n",
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D,\\\n",
    "MaxPooling1D,Dense,LSTM,Embedding\n",
    "from tensorflow.keras.preprocessing import \\\n",
    "sequence as ksequence\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.callbacks import \\\n",
    "ModelCheckpoint,ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def bin_accuracy(model,data_loader):\n",
    "    model.eval()\n",
    "    correct_pred,num_examples=0,0\n",
    "    with torch.no_grad():\n",
    "        for batch_ids,batch_data in enumerate(data_loader):\n",
    "            text,text_lengths=batch_data.text\n",
    "            logits=model(text,text_lengths)\n",
    "            predicted_labels=(torch.sigmoid(logits)>.5).long()\n",
    "            num_examples+=batch_data.label.size(0)\n",
    "            correct_pred+=(predicted_labels==\\\n",
    "                           batch_data.label.long()).sum()\n",
    "        return correct_pred.float()/num_examples*100\n",
    "def predict_sentiment(model,sentence):\n",
    "    model.eval()\n",
    "    tokenized=[tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed=[ttext.vocab.stoi[t] for t in tokenized]\n",
    "    length_tensor=torch.LongTensor([len(indexed)])\n",
    "    tensor=torch.LongTensor(indexed).to(dev)\n",
    "    tensor=tensor.unsqueeze(1)\n",
    "    prediction=torch.sigmoid(model(tensor,length_tensor))\n",
    "    return prediction.item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @import 'https://fonts.googleapis.com/css?family=Akronim&effect=3d';      \n",
       "    </style><h1 class='font-effect-3d' \n",
       "    style='font-family:Akronim; color:#33ffcc;'>\n",
       "    Data</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dhtml('Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading aclImdb_v1.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:08<00:00, 10.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "random_state=random.seed(12)\n",
    "ids='train valid test'.split()\n",
    "ttext=ttdata.Field(tokenize='spacy',\n",
    "                   include_lengths=True)\n",
    "tlabel=ttdata.LabelField(dtype=torch.float)\n",
    "train,test=ttds.IMDB.splits(ttext,tlabel)\n",
    "train,valid=train\\\n",
    ".split(random_state=random_state,split_ratio=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>valid</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>length</th>\n",
       "      <td>20000</td>\n",
       "      <td>5000</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        train  valid   test\n",
       "length  20000   5000  25000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([len(el) for el in [train,valid,test]],\n",
    "    index=ids,columns=['length']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "text matrix: torch.Size([154, 128])\n",
      "label vector: torch.Size([128])\n",
      "valid\n",
      "text matrix: torch.Size([61, 128])\n",
      "label vector: torch.Size([128])\n",
      "test\n",
      "text matrix: torch.Size([42, 128])\n",
      "label vector: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "random_seed=23\n",
    "torch.manual_seed(random_seed)\n",
    "vocab_size=20000; batch_size=128\n",
    "ttext.build_vocab(train,max_size=vocab_size)\n",
    "tlabel.build_vocab(train)\n",
    "num_clases=tlabel.vocab\n",
    "input_dim=len(ttext.vocab); output_dim=1\n",
    "train_loader,valid_loader,test_loader=\\\n",
    "ttdata.BucketIterator.splits((train,valid,test),\\\n",
    "batch_size=batch_size,sort_within_batch=True,device=dev)\n",
    "del train,test,valid\n",
    "dataloaders={ids[0]:train_loader,\n",
    "             ids[1]:valid_loader,\n",
    "             ids[2]:test_loader}\n",
    "for i in range(3):\n",
    "    print(ids[i])\n",
    "    for batch in dataloaders[ids[i]]:\n",
    "        print(f'text matrix: {batch.text[0].size()}')\n",
    "        print(f'label vector: {batch.label.size()}')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 25,000 movies reviews from IMDB, \n",
    "# labeled by sentiment (positive/negative)\n",
    "num_words,max_length,embedding_vector_len=\\\n",
    "10000,1000,32\n",
    "(x_train,y_train),(x_test,y_test)=\\\n",
    "imdb.load_data(path=\"imdb_full.pkl\",num_words=num_words,\n",
    "               skip_top=0,maxlen=max_length,seed=113,\n",
    "               start_char=1,oov_char=2,index_from=3)\n",
    "x_test,x_valid,y_test,y_valid=\\\n",
    "train_test_split(x_test,y_test,\n",
    "                 test_size=.5,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (12410,) (12409,)\n",
      "Label:  0\n",
      "Sequence of word indices: \n",
      " [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,x_valid.shape,x_test.shape)\n",
    "print('Label: ',y_train[1]) \n",
    "print('Sequence of word indices: \\n',x_train[1])\n",
    "px_train=ksequence\\\n",
    ".pad_sequences(x_train,maxlen=max_length)\n",
    "px_valid=ksequence\\\n",
    ".pad_sequences(x_valid,maxlen=max_length)\n",
    "px_test=ksequence\\\n",
    ".pad_sequences(x_test,maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @import 'https://fonts.googleapis.com/css?family=Akronim&effect=3d';      \n",
       "    </style><h1 class='font-effect-3d' \n",
       "    style='font-family:Akronim; color:#33ffcc;'>\n",
       "    Basic RNNs</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dhtml('Basic RNNs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tnn.Module):\n",
    "    def __init__(self,input_dim,embedding_dim,\n",
    "                 hidden_dim,output_dim):      \n",
    "        super().__init__()    \n",
    "        self.embedding=tnn.Embedding(input_dim,embedding_dim)\n",
    "        self.rnn=tnn.LSTM(embedding_dim,hidden_dim)\n",
    "        self.fc=tnn.Linear(hidden_dim,output_dim)    \n",
    "    def forward(self,text,text_length):\n",
    "        embedded=self.embedding(text)\n",
    "        packed=tnn.utils.rnn\\\n",
    "        .pack_padded_sequence(embedded,text_length)\n",
    "        packed_output,(hidden,cell)=self.rnn(packed)\n",
    "        return self.fc(hidden.squeeze(0)).view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmodel():\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(num_words,embedding_vector_len,\n",
    "                        input_length=max_length))\n",
    "    model.add(Conv1D(filters=32,kernel_size=3,\n",
    "                     padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))   \n",
    "    model.add(LSTM(32))    \n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='nadam',metrics=['accuracy'])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @import 'https://fonts.googleapis.com/css?family=Akronim&effect=3d';      \n",
       "    </style><h1 class='font-effect-3d' \n",
       "    style='font-family:Akronim; color:#33ffcc;'>\n",
       "    Training</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dhtml('Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed=34; learning_rate=1e-4\n",
    "torch.manual_seed(random_seed)\n",
    "embedding_dim=128; hidden_dim=256\n",
    "model=RNN(input_dim,embedding_dim,\n",
    "          hidden_dim,output_dim)\n",
    "model=model.to(dev)\n",
    "optimizer=torch.optim\\\n",
    ".Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch: 000/157 | Cost: 0.691940\n",
      "Epoch: 001/010 | Batch: 100/157 | Cost: 0.685660\n",
      "training acc: 58.80%\n",
      "valid acc: 58.10%\n",
      "Epoch: 002/010 | Batch: 000/157 | Cost: 0.679905\n",
      "Epoch: 002/010 | Batch: 100/157 | Cost: 0.570704\n",
      "training acc: 71.16%\n",
      "valid acc: 70.52%\n",
      "Epoch: 003/010 | Batch: 000/157 | Cost: 0.626943\n",
      "Epoch: 003/010 | Batch: 100/157 | Cost: 0.535192\n",
      "training acc: 76.06%\n",
      "valid acc: 74.00%\n",
      "Epoch: 004/010 | Batch: 000/157 | Cost: 0.462594\n",
      "Epoch: 004/010 | Batch: 100/157 | Cost: 0.498629\n",
      "training acc: 79.07%\n",
      "valid acc: 77.26%\n",
      "Epoch: 005/010 | Batch: 000/157 | Cost: 0.458864\n",
      "Epoch: 005/010 | Batch: 100/157 | Cost: 0.596968\n",
      "training acc: 80.91%\n",
      "valid acc: 78.56%\n",
      "Epoch: 006/010 | Batch: 000/157 | Cost: 0.373488\n",
      "Epoch: 006/010 | Batch: 100/157 | Cost: 0.402242\n",
      "training acc: 82.50%\n",
      "valid acc: 79.68%\n",
      "Epoch: 007/010 | Batch: 000/157 | Cost: 0.386685\n",
      "Epoch: 007/010 | Batch: 100/157 | Cost: 0.434860\n",
      "training acc: 80.03%\n",
      "valid acc: 77.26%\n",
      "Epoch: 008/010 | Batch: 000/157 | Cost: 0.425717\n",
      "Epoch: 008/010 | Batch: 100/157 | Cost: 0.367515\n",
      "training acc: 84.48%\n",
      "valid acc: 81.48%\n",
      "Epoch: 009/010 | Batch: 000/157 | Cost: 0.444618\n",
      "Epoch: 009/010 | Batch: 100/157 | Cost: 0.365021\n",
      "training acc: 84.78%\n",
      "valid acc: 81.44%\n",
      "Epoch: 010/010 | Batch: 000/157 | Cost: 0.396777\n",
      "Epoch: 010/010 | Batch: 100/157 | Cost: 0.461296\n",
      "training acc: 86.75%\n",
      "valid acc: 83.34%\n",
      "test acc: 82.34%\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch_ids,batch_data in enumerate(train_loader):\n",
    "        text,text_lengths=batch_data.text\n",
    "        logits=model(text,text_lengths)\n",
    "        cost=tnnf.binary_cross_entropy_with_logits(logits,\n",
    "                                                   batch_data.label)\n",
    "        optimizer.zero_grad()\n",
    "        cost.backward(); optimizer.step()\n",
    "        if not batch_ids%100:\n",
    "            print (f'Epoch: {epoch+1:03d}/{epochs:03d} | '\n",
    "                   f'Batch: {batch_ids:03d}/{len(train_loader):03d} | '\n",
    "                   f'Cost: {cost:.6f}')\n",
    "    with torch.set_grad_enabled(False):\n",
    "        print(f'training acc: '\n",
    "              f'{bin_accuracy(model,train_loader):.2f}%'\n",
    "              f'\\nvalid acc: '\n",
    "              f'{bin_accuracy(model,valid_loader):.2f}%')\n",
    "print(f'test acc: {bin_accuracy(model,test_loader):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-output": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.4585 - accuracy: 0.7676\n",
      "Epoch 00001: val_loss improved from inf to 0.29856, saving model to weights.best.hdf5\n",
      "196/196 [==============================] - 9s 48ms/step - loss: 0.4585 - accuracy: 0.7677 - val_loss: 0.2986 - val_accuracy: 0.8757 - lr: 0.0010\n",
      "Epoch 2/5\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.2317 - accuracy: 0.9117\n",
      "Epoch 00002: val_loss did not improve from 0.29856\n",
      "196/196 [==============================] - 9s 44ms/step - loss: 0.2316 - accuracy: 0.9117 - val_loss: 0.3044 - val_accuracy: 0.8761 - lr: 0.0010\n",
      "Epoch 3/5\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.1771 - accuracy: 0.9362\n",
      "Epoch 00003: val_loss did not improve from 0.29856\n",
      "196/196 [==============================] - 9s 44ms/step - loss: 0.1770 - accuracy: 0.9362 - val_loss: 0.3043 - val_accuracy: 0.8786 - lr: 0.0010\n",
      "Epoch 4/5\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.1474 - accuracy: 0.9490\n",
      "Epoch 00004: val_loss did not improve from 0.29856\n",
      "196/196 [==============================] - 9s 46ms/step - loss: 0.1474 - accuracy: 0.9490 - val_loss: 0.4038 - val_accuracy: 0.8646 - lr: 0.0010\n",
      "Epoch 5/5\n",
      "195/196 [============================>.] - ETA: 0s - loss: 0.1124 - accuracy: 0.9631\n",
      "Epoch 00005: val_loss did not improve from 0.29856\n",
      "196/196 [==============================] - 9s 45ms/step - loss: 0.1124 - accuracy: 0.9631 - val_loss: 0.3928 - val_accuracy: 0.8502 - lr: 0.0010\n",
      "388/388 [==============================] - 4s 11ms/step - loss: 0.2940 - accuracy: 0.8794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.293979287147522, 0.8794423341751099]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmodel=kmodel()\n",
    "fw='weights.best.hdf5'\n",
    "checkpointer=\\\n",
    "ModelCheckpoint(filepath=fw,verbose=2,\n",
    "                save_best_only=True)\n",
    "lr_reduction=\\\n",
    "ReduceLROnPlateau(monitor='val_loss',patience=10,\n",
    "                  verbose=2,factor=.5)\n",
    "history=kmodel.fit(px_train,y_train,epochs=5,batch_size=128,\n",
    "                   validation_data=(px_valid,y_valid),\n",
    "                   callbacks=[checkpointer,lr_reduction])\n",
    "kmodel.load_weights(fw)\n",
    "kmodel.evaluate(px_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @import 'https://fonts.googleapis.com/css?family=Akronim&effect=3d';      \n",
       "    </style><h1 class='font-effect-3d' \n",
       "    style='font-family:Akronim; color:#33ffcc;'>\n",
       "    Predictions</h1>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dhtml('Predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of being positive:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.873988687992096"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('probability of being positive:')\n",
    "sent='I really love this movie. The actor team here is so great!'\n",
    "predict_sentiment(model,sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5533997]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index=imdb.get_word_index()\n",
    "sent=sent.lower().replace('.','').replace('!','')\n",
    "sent_index=[]\n",
    "for word in sent.split():\n",
    "     sent_index.append(word2index[word])\n",
    "sent_index=ksequence.pad_sequences([sent_index],\n",
    "                                   maxlen=max_length)\n",
    "kmodel.predict(sent_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
